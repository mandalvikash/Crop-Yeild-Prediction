ðŸŒ¾ Explainable AI â€“ Model Comparison Report for Crop Yield Prediction
ðŸ” 1. Models Evaluated

We trained and tested the following Machine Learning models on the crop yield dataset:

1.Linear Regression
2.Lasso Regression
3.Ridge Regression
4.Decision Tree Regressor
5.K-Nearest Neighbors (KNN)
6.Gradient Boosting Regressor

We compared them using:

MAE (Mean Absolute Error) â†’ lower is better

RÂ² Score (Coefficient of Determination) â†’ higher is better

ðŸ¥‡ 2. Best Model: K-Nearest Neighbors (KNN)
Performance

MAE: 4,679 (lowest error)

RÂ² Score: 0.9846 (highest accuracy)


âœ” Why KNN is Best
Dataset has strong local patterns
Crops grown under similar conditions have very similar yields.
KNN uses â€œclosest neighborsâ€, so it captures these patterns extremely well.

No linear relationship required
Crop yield depends on rainfall, pesticides, temperature, crop type, etc.
These relationships are not linear, so KNN performs better than linear models.

Scaled features improve performance
After applying StandardScaler in preprocessing, KNN works extremely well.

â­ Simple Explanation

KNN predicts yield by looking at the most similar historical farming conditions.
Because our dataset contains many repeated or similar conditions (same country, same crop), KNN gives very accurate predictions.

ðŸ¥ˆ 3. Second Best Model: Decision Tree
Performance

MAE: 5,524

RÂ² Score: 0.9693

âœ” Why Decision Tree performs well

Non-linear decision boundaries
Trees split data into ranges (e.g., rainfall > 1000, temp < 20).
This works well for agriculture data.

Understands categorical splits naturally
Decision Trees work well even without scaling.

âŒ Why it is NOT as good as KNN

Trees can overfit on small variations.
Leaf nodes may contain few samples â†’ less stable predictions.
Single tree models do not generalize as strongly as KNN.

â­ Simple Explanation

Decision Trees learn rules such as:
If rainfall is high AND temp is moderate â†’ yield is high.
But sometimes trees create too many small rules, causing small errors.

ðŸš« 4. Why Gradient Boosting and Linear Models Perform Poorly
Linear Models (Linear, Lasso, Ridge)
They assume a straight-line relationship.
Real crop yield relationships are highly non-linear.
Thatâ€™s why all three have MAE around 29,000 and RÂ² around 0.74.

Gradient Boosting
Needs hyperparameter tuning to perform well.
Default settings underfit your dataset.
Thatâ€™s why MAE is very high (25,519) and accuracy low (0.75).

ðŸ“Š 5. Summary Table
Model	MAE â†“	RÂ² Score â†‘	Rank
KNN	4,679	0.9846	ðŸ¥‡ Best
Decision Tree	5,524	0.9693	ðŸ¥ˆ Second
Gradient Boosting	25,519	0.755	3rd Worst
Linear Regression	29,897	0.747	Poor
Lasso	29,883	0.747	Poor
Ridge	29,852	0.747	Poor

ðŸ’¡ 6. Final Explainable Conclusion
âœ” Best Model â€“ KNN
Because crop yield is influenced by multiple nonlinear factors, KNN finds the most similar historical conditions and predicts very accurately.

âœ” Second Best â€“ Decision Tree
It learns rule-based splits which match agricultural behavior but overfits slightly.

âœ” Why difference between KNN and Decision Tree?
Reason	KNN (Best)	Decision Tree (Second Best)
Uses similar real data points	âœ” Yes	âŒ No
Sensitive to local patterns	âœ” Strong	âœ” Medium
Smooth predictions	âœ” Smooth	âŒ Jumps due to splits
Overfitting	âŒ Low	âœ” Higher
Handles distance well	âœ” Yes	âŒ Not distance-based

KNN gives smoother, more stable predictions.
Decision Tree gives rule-based predictions but can overfit.